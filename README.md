# ğŸ§  Deep Q-Learning and Its Variants â€“ HW4 Interactive Showcase

This project presents an interactive web interface built with Flask to showcase and compare various Deep Q-Learning (DQN) algorithms as implemented in HW4 of a Deep Reinforcement Learning course.

It combines model training (in PyTorch / PyTorch Lightning), reward tracking, and real-time visualization via Plotly.

---

## ğŸ“¦ Features

-   ğŸ§ª **Train and evaluate**: Naive DQN, Double DQN, Dueling DQN, and Lightning DQN
-   ğŸ“‰ **Interactive reward curve** with zoom/range/scale toggle
-   ğŸŒ **Flask Web UI** with clean Bootstrap layout
-   ğŸ“Š **Live JSON-based data loading** for real-time result rendering

---

## ğŸ“ Project Structure

```bash
â”œâ”€â”€ app.py                       # Main Flask app
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ base.html               # Common layout template
â”‚   â”œâ”€â”€ index.html              # Landing page
â”‚   â”œâ”€â”€ report.html             # Theory + result summary
â”‚   â””â”€â”€ results.html            # Reward curve visualizations (interactive)
â”œâ”€â”€ static/
â”‚   â””â”€â”€ plots/                  # Reward curve images (optional)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ naive_rewards.json
â”‚   â”œâ”€â”€ double_rewards.json
â”‚   â”œâ”€â”€ dueling_rewards.json
â”‚   â””â”€â”€ lightning_rewards.json  # Episode reward lists per model
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ dqn_train.py            # PyTorch implementation of Naive/Double/Dueling
â”‚   â”œâ”€â”€ naive_train.py          # Baseline Naive DQN
â”‚   â””â”€â”€ lightning_train.py      # PyTorch Lightning version
â””â”€â”€ README.md                   # This file
```

---

## ğŸš€ How to Run

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

Make sure to include:

-   `flask`
-   `pytorch-lightning`
-   `matplotlib`
-   `plotly`

### 2. Train the models

Train and generate reward curves:

```bash
python training/naive_train.py
python training/dqn_train.py    # For double / dueling
python training/lightning_train.py
```

> This will generate reward plots (PNG) and `.json` files under `data/`

### 3. Launch the Flask server

```bash
python app.py
```

Visit: [http://localhost:5000](http://localhost:5000)

---

## ğŸ“„ Page Overview

| Route                  | Purpose                               |
| ---------------------- | ------------------------------------- |
| `/`                    | Home page (navigation)                |
| `/report`              | Theoretical explanation (HW4-1~4)     |
| `/results`             | Interactive reward plot comparison    |
| `/api/rewards/<model>` | JSON API for each modelâ€™s reward data |

---

## ğŸ” Model Summary

| Model         | Technique                       | Benefit                               |
| ------------- | ------------------------------- | ------------------------------------- |
| Naive DQN     | Basic Q-learning                | Simple, fast but unstable             |
| Double DQN    | Decoupled target selection      | Reduces overestimation bias           |
| Dueling DQN   | Value & Advantage decomposition | More stable for value-dominant states |
| Lightning DQN | PyTorch Lightning + scheduler   | Cleaner training, easier to extend    |

---

## ğŸ“· Screenshots

-   ğŸ“Š `/results`: Reward curves with zoom, pan, log scale toggle
-   ğŸ§  `/report`: Architecture comparisons and loss formula

---

## ğŸ“Œ Notes

-   Data files are loaded dynamically â€“ missing reward JSONs will silently fail
-   You can export plots as images via the Plotly menu

---

## ğŸ¤– Author

Developed as part of a university course on Deep Reinforcement Learning (Spring 2025).

---

## ğŸ§  License

MIT License

Feel free to adapt and extend this code for your own RL experiments!
