# üß† Deep Q-Learning and Its Variants ‚Äì HW4 Interactive Showcase

This project presents an interactive web interface built with Flask to showcase and compare various Deep Q-Learning (DQN) algorithms as implemented in HW4 of a Deep Reinforcement Learning course.

It combines model training (in PyTorch / PyTorch Lightning), reward tracking, and real-time visualization via Plotly.

---

## üì¶ Features

-   üß™ **Train and evaluate**: Naive DQN, Double DQN, Dueling DQN, and Lightning DQN
-   üìâ **Interactive reward curve** with zoom/range/scale toggle
-   üåê **Flask Web UI** with clean Bootstrap layout
-   üìä **Live JSON-based data loading** for real-time result rendering

---

## üìÅ Project Structure

```bash
‚îú‚îÄ‚îÄ app.py                       # Main Flask app
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ base.html               # Common layout template
‚îÇ   ‚îú‚îÄ‚îÄ index.html              # Landing page
‚îÇ   ‚îú‚îÄ‚îÄ report.html             # Theory + result summary
‚îÇ   ‚îî‚îÄ‚îÄ results.html            # Reward curve visualizations (interactive)
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îî‚îÄ‚îÄ plots/                  # Reward curve images (optional)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ naive_rewards.json
‚îÇ   ‚îú‚îÄ‚îÄ double_rewards.json
‚îÇ   ‚îú‚îÄ‚îÄ dueling_rewards.json
‚îÇ   ‚îî‚îÄ‚îÄ lightning_rewards.json  # Episode reward lists per model
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ dqn_train.py            # PyTorch implementation of Naive/Double/Dueling
‚îÇ   ‚îú‚îÄ‚îÄ naive_train.py          # Baseline Naive DQN
‚îÇ   ‚îî‚îÄ‚îÄ lightning_train.py      # PyTorch Lightning version
‚îî‚îÄ‚îÄ README.md                   # This file
```

---

## üöÄ How to Run

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

Make sure to include:

-   `flask`
-   `pytorch-lightning`
-   `matplotlib`
-   `plotly`

### 2. Train the models

Train and generate reward curves:

```bash
python training/naive_train.py
python training/dqn_train.py    # For double / dueling
python training/lightning_train.py
```

> This will generate reward plots (PNG) and `.json` files under `data/`

### 3. Launch the Flask server

```bash
python app.py
```

Visit: [http://localhost:5000](http://localhost:5000)

---

## üìÑ Page Overview

| Route                  | Purpose                               |
| ---------------------- | ------------------------------------- |
| `/`                    | Home page (navigation)                |
| `/report`              | Theoretical explanation (HW4-1~4)     |
| `/results`             | Interactive reward plot comparison    |
| `/api/rewards/<model>` | JSON API for each model‚Äôs reward data |

---

## üîç Model Summary

| Model         | Technique                       | Benefit                               |
| ------------- | ------------------------------- | ------------------------------------- |
| Naive DQN     | Basic Q-learning                | Simple, fast but unstable             |
| Double DQN    | Decoupled target selection      | Reduces overestimation bias           |
| Dueling DQN   | Value & Advantage decomposition | More stable for value-dominant states |
| Lightning DQN | PyTorch Lightning + scheduler   | Cleaner training, easier to extend    |

---

## üìå Notes

-   Data files are loaded dynamically ‚Äì missing reward JSONs will silently fail
-   You can export plots as images via the Plotly menu

---

## ü§ñ Author

Developed as part of a university course on Deep Reinforcement Learning (Spring 2025).

---

## üß† License

MIT License

Feel free to adapt and extend this code for your own RL experiments!
